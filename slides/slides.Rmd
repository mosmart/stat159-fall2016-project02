---
title: "Predictive Modeling"
author: "Morgan Smart and Lauren Hanlon"
date: "October 31, 2016"
output: ioslides_presentation
---

```{r setup 3, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r, echo=FALSE}
library(stargazer)
library(knitr)
load('../data/eda-output.RData')
load('../data/osl.RData')
load('../data/ridge-cv.RData')
load('../data/lasso-cv.RData')
load('../data/pcr-cv.RData')
load('../data/plsr-cv.RData')
load('../data/regression-results.RData')
```

# Overview

## Overview

- This presentation gives an overview of how different regression models can be applied to a dataset to predict an attribute
- We will be using the Credit dataset from "An Introduction to Statistical Learning" by James et all.

# Data

##Data
- `X`: row index
- `Income`: income (in thousands of dollars)
- `Limit`: credit limit
- `Rating`: credit rating
- `Cards`: number of credit cards
- `Age`: age
- `Education`: years of education
- `Gender`: gender
- `Student`: student status
- `Married`: marital status
- `Ethnicity`: Caucasian, African American or Asian
- `Balance`: average credit card debt

## Quantitative Data {.flexbox .vcenter}

First, some summary statistics about our data.

```{r results='asis', echo=FALSE, fig.align="center"}
stargazer(sd_range, title = 'Summary statistics quantitative variables', type='html', header=FALSE, summary=FALSE)
```

- Large ranges for `Income` (in thousands), `Limit` and `Rating`
- `Balance` also has a large spread, with a range of $`r sd_range["Balance", "Range"]`$

## Qualitative Data {.flexbox .vcenter}

Next, we look at the qualitative variables and their relative frequencies.

```{r, out.width = "200px", echo=FALSE, fig.align="center, fig.show='hold'"}
knitr::include_graphics("../images/barchart-gender.png")
knitr::include_graphics("../images/barchart-student.png")
```

```{r, out.width = "200px", echo=FALSE, fig.align="center, fig.show='hold'"}
knitr::include_graphics("../images/barchart-married.png")
knitr::include_graphics("../images/barchart-ethnicity.png")
```

## Assocation between Balance and predictors

Visual representation of the correlation between `Balance` and each variable. 

```{r, out.width = "200px", echo=FALSE, fig.align="center"}
knitr::include_graphics("../images/scatterplot-matrix.png")
```
- Correlation between `Balance` and `Limit` and `Rating` appears to be linear 

- Relationship between `Balance` and the other predictors less of a non-linear relationship

# Methodology

## Methods

_Ordinary Least Squares_: OLS is an approach to predicting a quantitative response $Y$ based on a multiple predictor variables $X_1$ through $X_p$, where $Y$ and $X_1$ through $X_p$ are vectors and each value in each $X_{ij}$ ($x_{ij}$) has a corresponding value in $Y$ ($y_i$).

_Shrinkage Methods_: Shrinkage is a general technique to improve a least-squares estimator which consists in reducing the variance by adding constraints on the value of coefficients.

- Ridge regression

- Lasso regression

## Methods (cont.)

_Dimension Reduction Methods_: dimension reduction is the process of reducing the number of random variables under consideration, via obtaining a set of principal variables.

- Principal Components regression (PCR)

- Partial Least Squares regression (PLSR)

# Analysis

## OSL

Our first method was OSL Regression. Below is the table of coefficients of each predictor when regressed against `Balance`

```{r results='asis', echo=FALSE}
library(stargazer)
stargazer(osl_summary_fit$coefficients, title = 'Coefficient estimates OSL', type='html', header=FALSE, summary=FALSE)
```

## OSL Results

- `Income`, `Education`, `Gender: Female`, and `Married: Yes` all have a negative relationship with `Balance`
- Increase in any one of those categories would result in a decrease in `Balance`.

## Lasso

LASSO regression: fitting the model using out training data and cross-validating to determine the lambda value that minimizes mean-squared error.

```{r, out.width = "300px", echo=FALSE, fig.align="center"}
knitr::include_graphics("../images/lasso-cv.png")
```

From this cross-validation, we found this minimum lambda value to be `r lasso_lambda`.

## Ridge Regression

We also ran a Ridge regression using the same methods as the LASSO regression. First we fit the model using out training data and cross-validated to determine the lambda value that minimizes mean-squared error: `r ridge_lambda`.

```{r, out.width = "300px", echo=FALSE, fig.align="center"}
knitr::include_graphics("../images/ridge-cv.png")
```

## Principle Component Analysis (PCA)

PCR fits the data the same way, but we now solve for the number of components with the smallest Predictive Residual Error Sum of Squares ($PRESS$).

```{r, out.width = "300px", echo=FALSE, fig.align="center"}
knitr::include_graphics("../images/pcr-cv.png")
```

Fom our cross-validation, we find the number of components with the minimum $PRESS$ value to be `r pcr_ncomp`.

## PLSR

Like PLSR, we first fit the model using our training data and cross-validated to determine the number of components with the minimum $PRESS$: `r plsr_ncomp`.

```{r, out.width = "300px", echo=FALSE, fig.align="center"}
knitr::include_graphics("../images/plsr-cv.png")
```

# Results

## Regression coefficients for all methods

```{r kable, echo=FALSE}
kable(coefs_df)
```

### Plotting the official coefficients

To get a better understanding of how to coefficient values differ across models, we plot the coefficient values for each model in the following overlapping line chart.

``` {r, echo = FALSE}
par(mar=c(7, 5, 2, 2))
matplot(coefs_df, type = "l", xaxt = "n", ylab = "Coefficient Estimate", col = c(1:5))
axis(1, at = 1:9, labels = row.names(coefs_df), cex.axis = 0.7, las = 2)
legend("topright", inset=.05, legend=c("OLS", "Lasso", "Ridge", "PCR", "PLSR"), pch=1, col=c(1:5), horiz=FALSE, bty = "n")
title("Official Coefficients")
```

Coefficients of PCR are the most stable and most different than that of the other models. OSL, Ridge, LASSO, and PLSR all estimate similar coefficient values for each feature.

## MSE values for the regressions

To determine which model has the most predictive power of `Balance`, we compare the MSE of the various models.

```{r, echo=FALSE}
kable(mse_df)
```

Because the Ridge coefficients fall in line with that of 3 other models (LASSO, PLSR, and OLS) and because it has the minimum MSE, we decide that of the models ran, Ridge has the best predictability of `Balance`.

# Conclusion

## Conclusion

And the winning model is... Ridge!

Of the models assessed in this analysis, we find that the Ridge model has the best predictive power of `Balance` because it has the lowest MSE of all the model and its estimated beta coefficients are consistant with the estimated coefficients of LASSO, OLS and PLSR