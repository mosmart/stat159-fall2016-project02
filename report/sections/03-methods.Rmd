
To understand what features are most predictive of Balance, we tested using 5 models: Ordinary Least Squares (OLS), LASSO Regression, Ridge Regression, Principle Component Regression (PCR) and Partial Least Squares Regression (PLSR). OLS is an approach to predicting a quantitative response $Y$ based on a multiple predictor variables $X_1$ through $X_p$, where $Y$ and $X_1$ through $X_p$ are vectors and each value in each $X_{ij}$ ($x_{ij}$) has a corresponding value in $Y$ ($y_i$). The model assumes that the relationship between every $X_i$ and $Y$ is linear and that each $X_j$ isn't correlated with any other $X_j$. OLS can be written as $Y \approx \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p + \epsilon$, where $\beta_0$ is the intercept and $\beta_1$ through $\beta_p$ are the slopes of their corresponding predictor variable $X_j$. The beta values are all constants, unknowns, and together are the model coefficients. The interpretation of $\beta_0$ is the expected mean value of $Y$ without a predictor variable and the interpretation of $\beta_1$ through $\beta_p$ is the change in $Y$ for a unit increase in the beta's corresponding $X_j$. Although the betas are unknown, we can estimate them using the OLS model: solving for the intercept and slopes that produce the plane closest to each point ($x_{ij}$,$y_i$) in each $X_j$,$Y$, which is minimizing the residual sum of squares ($RSS$). Once we have an estimate for the betas, we can compute using OLS to determine the strength of the relationship between each $X_j$ and $Y$, if the relationship is statistically significant, and asses how accurately the model predicts the relationship.

LASSO relies upon the OLS model but uses a different procedure in order to estimating the coefficients $\beta_1$ through $\beta_p$. This new procedure sets a number of the betas to exactly zero, making the LASSO in a sense less flexible than OLS. However, the LASSO is more interpretable than OLS because in the final model, $Y$ is only related to a small subset of the $X$'s -- those with nonzero beta coefficients.

Ridge Regression also relies upon the OLS model, but like LASSO, it uses a different procedure in order to estimate the betas. Rather than estimating the betas that minimize $RSS$ like OLS, Ridge imposes a tuning parameter $\lambda$ to the sum of the betas (together called the shrinkage penalty) and adds this shrinkage penalty to RSS before solving for the beta values. The shrinkage penalty is small when the betas are close to zero; thus, it shrinks the estimated betas toward zero. The tuning parameter is calculated separately from the model, controling the impact of both itself and the shrinkage penalty on the beta estimates. If the tuning parameter is zero, the penalty term has no effect and Ridge will produce the same betas as OLS. As the tuning parameter appoaches infinity, the penalty term's impact increases and the Ridge beta estimates approach zero. Unlike LASSO, the beta coefficients produced by Ridge will never be exactly zero, and thus the model is not as easy to interpret.

PCR is a dimension reduction technique for regression that utilizes Principle Component Analysis (PCA). PCA uses an orthogonal transformation to convert a set of observations of potentially correlated variables into a set of uncorrelated variables, called principal components. These principal components are generated by identifying linear combinations, or directions, that best represent the $X$'s. The number of generated components is always less than or equal to the number of original variables. The PCA transformation occurs is defined in a way that causes the first principal component to have the largest possible variance and each succeeding component to have the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors are an uncorrelated orthogonal basis set. Additionally, PCA is sensitive to the relative scaling of the original variables. As applied to PCR, we assume that the directions in which $X_1$ through $X_p$ show the most variation are the directions that are associated with $Y$ and thus a model using the components rather than the $X$'s will be better because the components account for the variability of the $X$'s while reducing the dimensionality, reducing overfitting.

PLSR is similar to PCR but it generates components in a supervised way. In PCR, the linear combinations identified to generate the components are identified in an unsupervised way, since the response $Y$ isn't used to help determine the linear combinations. Thus, in PCR, there is no guarantee that the components that best explain the predictors will also be the best components to use for predicting the response. PLSR uses $Y$ in order to identify new features that approximate the old features and are related to $Y$ -- attempting to find components that help explain both the response and the predictors.

To solve for $\lambda$ in LASSO and Ridge and to solve for the number of components in PCR and PLSR, we use cross-validation. Cross-validation is a model validation technique used to assess how the results of an analysis will generalize to an independent data set. In this analysis, we use cross-validation to improve prediction to estimate how accurately our various models will perform in practice. To cross-validate a model, we fit its required estimated parameters using a training dataset and then test the created model using an unseen test dataset. If the model performs well using the test dataset, we are more confident that this model is accurate (since it produces good prediction of data not used to fit the model).


